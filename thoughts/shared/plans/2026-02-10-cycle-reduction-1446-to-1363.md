# Cycle Reduction: 1446 → <1363 Implementation Plan

## Overview

Reduce the VLIW kernel from 1446 cycles to below 1363 (83 cycles = 5.7%) by attacking the load-engine bottleneck and increasing interleave parallelism. The key insight: ~53 `load.const` operations waste the scarce load engine (2 slots/cycle, 92% saturated) on constant initialization that can instead use the idle `flow.add_imm` instruction (1 slot/cycle, only 35% saturated). This frees load bandwidth, lowers the hard minimum, AND frees 31 scratch words to enable 32-group interleaving that eliminates the costly 7-group tail in gather-depth rounds.

## Current State Analysis

**Cycle count**: 1446 (need < 1363, gap = 83)

**Hard resource minimums** (from total ops in body):
| Resource | Total ops | Slots/cycle | Min cycles | Status |
|----------|-----------|-------------|-----------|--------|
| VALU | 8,029 | 6 | 1,339 | Near binding |
| **Load** | **~2,654** | **2** | **~1,327–1,347** | **BINDING** |
| Flow | 512 | 1 | 512 | Underutilized |
| ALU | ~70 | 12 | 6 | Idle |
| Store | 32 | 2 | 16 | Idle |

**Overhead**: 1446 − 1347 = **99 cycles** from scheduling inefficiency.

**Scratch**: 1512/1536 used (24 free).

### Key Discoveries:
- Load engine carries ~53 `const` ops (header scalars + offset constants) that don't need memory access — `perf_takehome.py:875-880,1272-1277`
- `flow.add_imm(dest, src, imm)` computes `scratch[dest] = scratch[src] + imm` — semantically identical to `load.const` when src holds 0 — `problem.py:306-307`, `frozen_problem.py:306-307`
- Scratch is initialized to all zeros (`problem.py:108`), so an unwritten address reads as 0
- Offset constants (31 words) are only needed for `ALU.add` addressing; `add_imm` eliminates both the const AND the ALU op
- Only depth-2 uses `vec_val_save` (3rd group temp); depths 0,1,3+ need only 2 temps — `perf_takehome.py:1029-1262`
- `interleave_groups >= 30` was previously impossible due to scratch limits — `docs/OPTIMIZATION_MASTER_SUMMARY.md:229`

## Desired End State

- All submission tests pass (`python tests/submission_tests.py`)
- Cycle count < 1363 (passing `test_opus45_improved_harness`)
- 9/9 tests passing
- Debug mode (`emit_debug=True`) continues to work unchanged

### Verification:
```bash
python tests/submission_tests.py
python perf_takehome.py Tests.test_kernel_cycles
```

## What We're NOT Doing

- NOT changing the hash algorithm (12 VALU slots/hash is already optimal)
- NOT preloading depth-3 nodes (adds too much VALU, pushes VALU min > 1363)
- NOT replacing depth-2 flow.vselect with VALU mux (adds 320 VALU, exceeds budget)
- NOT changing the scheduler algorithm itself (only tuning parameters)
- NOT modifying any files in `tests/`

## Implementation Approach

Three complementary optimizations applied incrementally:
1. Move constant initialization from load engine to flow engine (reduces load minimum)
2. Eliminate offset constant scratch allocations (frees scratch + more load savings)
3. Use freed scratch for tiered interleave groups to reach 32 total (eliminates tail overhead)

Each phase is independently testable and revertable.

---

## Phase 1: Convert Constant Loads to flow.add_imm

### Overview
Replace all `("load", ("const", addr, val))` in the submission-mode body with `("flow", ("add_imm", addr, zero_base, val))`. This moves ~53 constant-initialization ops from the load engine (92% saturated) to the flow engine (35% saturated).

### Changes Required:

#### 1.1 Allocate zero_base scratch address

**File**: `perf_takehome.py:845-846`
**Changes**: Add a dedicated zero-base address that is never written, so it remains 0 throughout execution.

```python
# After line 845 (header = []):
# Allocate a never-written scratch address for use as add_imm zero source.
# Scratch is initialized to 0 by the Machine, so reading this always yields 0.
zero_base = self.alloc_scratch("zero_base") if not self.emit_debug else None
```

#### 1.2 Modify header_scratch_const to use add_imm

**File**: `perf_takehome.py:875-880`
**Changes**: Replace const load with add_imm in submission mode.

```python
def header_scratch_const(val, name=None):
    if val not in self.const_map:
        addr = self.alloc_scratch(name)
        self.const_map[val] = addr
        if zero_base is not None:
            header.append(("flow", ("add_imm", addr, zero_base, val)))
        else:
            header.append(("load", ("const", addr, val)))
    return self.const_map[val]
```

#### 1.3 Modify header init var loads to use add_imm

**File**: `perf_takehome.py:869-872`
**Changes**: Replace the header-index const loads.

```python
for idx_i, (name, header_idx) in enumerate(init_vars):
    htmp = header_tmp_addrs[idx_i]
    if zero_base is not None:
        header.append(("flow", ("add_imm", htmp, zero_base, header_idx)))
    else:
        header.append(("load", ("const", htmp, header_idx)))
    header.append(("load", ("load", self.scratch[name], htmp)))
```

### Success Criteria:

#### Automated Verification:
- [x] All 8 correctness seeds pass: `python tests/submission_tests.py CorrectnessTests`
- [x] Cycle count ≤ 1446 (should improve, not regress): `python tests/submission_tests.py SpeedTests` — **1440 cycles (−6)**
- [x] Debug mode still works: `python perf_takehome.py Tests.test_kernel_trace` — 1489 debug cycles

#### Manual Verification:
- [ ] Confirm cycle count dropped (check CYCLES output)
- [ ] Run utilization analysis to verify load% decreased and flow% increased

**Implementation Note**: After completing this phase and all automated verification passes, pause here for manual confirmation from the human that the manual testing was successful before proceeding to the next phase.

---

## Phase 2: Eliminate Offset Constants via add_imm Addressing

### Overview
Replace the batch-loading and epilogue address pattern `const_load + ALU.add + vload/vstore` with the more direct `flow.add_imm + vload/vstore`. This eliminates 31 scratch allocations for offset constants AND removes 31 load ops + 64 ALU ops from the body.

### Changes Required:

#### 2.1 Skip offset constant allocation in submission mode

**File**: `perf_takehome.py:1264-1277`
**Changes**: Guard the offset allocation block to only run in debug mode.

```python
# Pre-allocate offset constant scratch addresses and emit loads into body
# so VLIW scheduler can pair them (instead of single-load cycles via self.add)
offset_addrs = {}
if self.emit_debug or zero_base is None:
    all_offsets = set()
    for base in range(0, vec_count, VLEN):
        all_offsets.add(base)
    for i in range(vec_count, batch_size):
        all_offsets.add(i)
    for off in sorted(all_offsets):
        if off not in self.const_map:
            addr = self.alloc_scratch()
            self.const_map[off] = addr
        offset_addrs[off] = self.const_map[off]
        body.append(("load", ("const", offset_addrs[off], off)))
```

#### 2.2 Use add_imm for batch value loading

**File**: `perf_takehome.py:1279-1299`
**Changes**: Replace ALU addressing with add_imm for submission mode.

```python
for base in range(0, vec_count, VLEN):
    if use_idx_mem:
        if zero_base is not None:
            body.append(("flow", ("add_imm", tmp_addr, self.scratch["inp_indices_p"], base)))
        else:
            body.append(("alu", ("+", tmp_addr, self.scratch["inp_indices_p"], offset_addrs[base])))
        body.append(("load", ("vload", idx_arr + base, tmp_addr)))
    if zero_base is not None:
        body.append(("flow", ("add_imm", tmp_addr_b, self.scratch["inp_values_p"], base)))
    else:
        body.append(("alu", ("+", tmp_addr_b, self.scratch["inp_values_p"], offset_addrs[base])))
    body.append(("load", ("vload", val_arr + base, tmp_addr_b)))
```

#### 2.3 Use add_imm for epilogue value storing

**File**: `perf_takehome.py:1349-1370`
**Changes**: Same pattern for the store-back phase.

```python
for base in range(0, vec_count, VLEN):
    if use_idx_mem:
        if zero_base is not None:
            body.append(("flow", ("add_imm", tmp_addr, self.scratch["inp_indices_p"], base)))
        else:
            body.append(("alu", ("+", tmp_addr, self.scratch["inp_indices_p"], offset_addrs[base])))
        body.append(("store", ("vstore", tmp_addr, idx_arr + base)))
    if zero_base is not None:
        body.append(("flow", ("add_imm", tmp_addr_b, self.scratch["inp_values_p"], base)))
    else:
        body.append(("alu", ("+", tmp_addr_b, self.scratch["inp_values_p"], offset_addrs[base])))
    body.append(("store", ("vstore", tmp_addr_b, val_arr + base)))
```

#### 2.4 Same for scalar tail batch loading/storing (lines 1290-1299, 1360-1369)

Apply the same `add_imm` pattern for the scalar tail (`range(vec_count, batch_size)`). Although this path has zero iterations when batch_size=256, it must remain correct for other batch sizes.

### Success Criteria:

#### Automated Verification:
- [x] All 8 correctness seeds pass: `python tests/submission_tests.py CorrectnessTests`
- [x] Cycle count: 1443 (−3 from baseline 1446; +3 from Phase 1's 1440 — flow.add_imm addressing adds minor scheduling overhead, but frees 31 scratch words for Phase 3)
- [x] Debug mode still works: `python perf_takehome.py Tests.test_kernel_trace` — 1489 debug cycles

#### Manual Verification:
- [ ] Confirm cycle count dropped further
- [ ] Verify scratch_ptr value is ~1482 (1512 - 31 + 1)

**Implementation Note**: After completing this phase and all automated verification passes, pause here for manual confirmation before proceeding to Phase 3.

---

## Phase 3: Tiered Interleave Groups (29 Full + 3 Extra = 32 Total)

### Overview
Use the 55 freed scratch words (31 from offset constants + 24 original headroom) to allocate 3 additional interleave groups with only 2 temp registers each (vec_node_val + vec_addr, no vec_val_save). This enables 32-group single-pass processing for depths 0, 1, and 3+, eliminating the costly 7-group tail in gather rounds.

### Changes Required:

#### 3.1 Repurpose constructor parameters

**File**: `perf_takehome.py:268-296`
**Changes**: Change default interleave_groups from 25 to 32. Reinterpret interleave_groups_early as "number of full 3-reg groups" (for depth 2).

```python
class KernelBuilder:
    def __init__(
        self,
        emit_debug: bool = False,
        interleave_groups: int = 32,           # was 25; total groups (2-reg extras included)
        interleave_groups_early: int | None = 29,  # full 3-reg groups (for depth 2)
        depth2_select_mode: str = "flow_vselect",
        idx_branch_mode: str = "flow_vselect",
        trace_phase_tags: bool = False,
        scheduler_profile: bool = False,
        scheduler_crit_weight: int = 1024,
        scheduler_engine_bias: dict[str, int] | None = None,
    ):
```

#### 3.2 Allocate tiered group registers

**File**: `perf_takehome.py:1009-1021`
**Changes**: Allocate full groups up to interleave_groups_early, then 2-reg groups up to interleave_groups.

```python
interleave_groups_total = self.interleave_groups
interleave_groups_full = self.interleave_groups_early
# In non-debug submission mode with freed scratch, we can have extra 2-reg groups
if not self.emit_debug and zero_base is not None:
    group_regs = []
    for g in range(interleave_groups_total):
        entry = {
            "vec_node_val": self.alloc_scratch(f"vec_node_val_g{g}", VLEN),
            "vec_addr": self.alloc_scratch(f"vec_addr_g{g}", VLEN),
        }
        if g < interleave_groups_full:
            entry["vec_val_save"] = self.alloc_scratch(f"vec_val_save_g{g}", VLEN)
        else:
            entry["vec_val_save"] = None  # 2-reg group, saves 8 words
        group_regs.append(entry)
else:
    # Debug mode or legacy: all groups have 3 regs
    max_groups = max(self.interleave_groups, self.interleave_groups_early)
    group_regs = []
    for g in range(max_groups):
        group_regs.append({
            "vec_node_val": self.alloc_scratch(f"vec_node_val_g{g}", VLEN),
            "vec_addr": self.alloc_scratch(f"vec_addr_g{g}", VLEN),
            "vec_val_save": self.alloc_scratch(f"vec_val_save_g{g}", VLEN),
        })
```

#### 3.3 Update round loop to use tiered groups

**File**: `perf_takehome.py:1301-1307`
**Changes**: Depth 2 uses only full groups; all other depths use all groups.

```python
for round in range(rounds):
    depth = round % (forest_height + 1)
    if not self.emit_debug and zero_base is not None:
        # Tiered mode: depth 2 needs vec_val_save (full groups only)
        if depth == 2:
            regs_list = group_regs[:interleave_groups_full]
        else:
            regs_list = group_regs[:interleave_groups_total]
    else:
        # Legacy mode
        regs_list = (
            group_regs[:self.interleave_groups_early]
            if depth <= 2
            else group_regs[:self.interleave_groups]
        )
```

### Scratch Budget Verification:

| Item | Words | Running Total |
|------|-------|--------------|
| Base (before changes) | 1512 | 1512 |
| + zero_base (Phase 1) | +1 | 1513 |
| − offset constants (Phase 2) | −31 | 1482 |
| + 3 extra groups × 2 regs × 8 | +48 | 1530 |
| **Remaining** | | **6 words** |

Total: 1530 / 1536 ✓

### Success Criteria:

#### Automated Verification:
- [ ] All 8 correctness seeds pass: `python tests/submission_tests.py CorrectnessTests`
- [ ] Cycle count < 1363: `python tests/submission_tests.py SpeedTests`
- [ ] No scratch overflow (assert at `perf_takehome.py:679`)

#### Manual Verification:
- [ ] Confirm cycle count is < 1363
- [ ] Run utilization analysis to verify load% dropped and scheduling improved
- [ ] Verify depth-3+ rounds process all 32 chunks in single pass (no tail)

**Implementation Note**: After completing this phase and all automated verification passes, pause here for manual confirmation. If cycle count is still above 1363, proceed to Phase 4.

---

## Phase 4: Parameter Sweep + Scheduler Tuning

### Overview
After structural changes in phases 1-3, the scheduling profile changes significantly. Sweep interleave parameters and scheduler settings to find the new optimum.

### Changes Required:

#### 4.1 Parameter sweep script

Create a sweep that tests combinations of:
- `interleave_groups`: [28, 29, 30, 31, 32]
- `interleave_groups_early`: [26, 27, 28, 29]
- `scheduler_crit_weight`: [512, 1024, 2048, 4096]
- `idx_branch_mode`: ["flow_vselect", "alu_branch"]

For each combination, run correctness + cycle count.

#### 4.2 Try splitting hash parallel pairs

In `build_hash_vec` (line 733-743), the parallel pair `[op1, op3]` is bundled as one 2-slot op. Try splitting into separate 1-slot ops to give the scheduler more flexibility:

```python
# Current (bundled):
slots.append(("valu", [(op1, tmp1, ...), (op3, tmp2, ...)]))
slots.append(("valu", (op2, val_hash_addr, tmp1, tmp2)))

# Alternative (split):
slots.append(("valu", (op1, tmp1, ...)))
slots.append(("valu", (op3, tmp2, ...)))
slots.append(("valu", (op2, val_hash_addr, tmp1, tmp2)))
```

This changes the scheduler's view: instead of one 2-slot op, it sees two 1-slot ops that can be individually scheduled.

### Success Criteria:

#### Automated Verification:
- [ ] Best config passes all correctness tests
- [ ] Cycle count < 1363
- [ ] Results logged for analysis

#### Manual Verification:
- [ ] Review parameter sweep results
- [ ] Confirm best config is set as default

---

## Testing Strategy

### Unit Tests:
- `python tests/submission_tests.py CorrectnessTests` — 8 random seeds
- `python perf_takehome.py Tests.test_kernel_cycles` — cycle count
- `python perf_takehome.py Tests.test_kernel_trace` — debug correctness

### Integration Tests:
- Full submission suite: `python tests/submission_tests.py`
- All 9 speed thresholds checked

### Manual Testing Steps:
1. After each phase, run full submission tests
2. Compare cycle count before and after
3. If regression, revert and investigate

## Performance Considerations

**Expected resource profile after all phases:**
| Resource | Before | After | Change |
|----------|--------|-------|--------|
| Load ops | ~2,654 | ~2,570 | −84 (consts + offsets removed) |
| Flow ops | 512 | ~598 | +86 (add_imm replacements) |
| VALU ops | 8,029 | 8,029 | unchanged |
| ALU ops | ~70 | ~6 | −64 (batch/epilogue addressing) |
| Load min | ~1,347 | ~1,285 | **−62 cycles** |
| VALU min | 1,339 | 1,339 | unchanged |
| **Binding** | **Load** | **VALU** | **Shifted to less-saturated engine** |

Interleave improvement (Phase 3):
| Depth | Before (groups) | After (groups) | Passes | Tail |
|-------|----------------|----------------|--------|------|
| 0, 1 | 29 | 32 | 1 | 0 |
| 2 | 29 | 29 | 2 (29+3) | 3 |
| 3+ | 25 | 32 | **1** | **0** (was 7) |

## References

- Research: `thoughts/shared/research/2026-02-10-vliw-kernel-optimization-codebase.md`
- Optimization history: `docs/OPTIMIZATION_MASTER_SUMMARY.md`
- ISA: `problem.py:48-60,219-335` (slot limits, engine operations)
- Frozen simulator: `tests/frozen_problem.py:306-307` (add_imm semantics)
- Kernel: `perf_takehome.py:760-1376` (build_kernel)
- Scheduler: `perf_takehome.py:450-619` (_schedule_vliw)

---

*Plan created: 2026-02-10 | Target: < 1363 cycles from 1446 | Approach: load→flow const migration + tiered interleave*
